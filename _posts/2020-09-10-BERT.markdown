---
layout: post
title: All about BERT
tags: [frontpage, jekyll, blog]
image: '/images/posts/7.jpg'
---

Homonyms are words that have the same spelling but have different meanings. Let's look at a few examples:

* He will __address__ the public about the recent news.
* Please give me your __address__.

* What is the __current__ update on the coronavirus?
* The river __current__ is strong.

* __Rock__ is my favorite genre.
* Let's play __rock__ paper scissors!

---
The English language has a lot of homonyms. We want computers to understand this nuisance. BERT does a really good job capturing the contextual differences between the examples. BERT is short for Bidirectional Encoder Representations from Transformers. It's a language model algorithm for solving Natural Language Processing (NLP) problems. It was developed by researchers at Google AI Language in 2018 and released to the public as an open-source language model in this [paper](https://arxiv.org/pdf/1810.04805.pdf). At the time, BERT achieved state-of-the-art performance in several NLP problems such as Classification, Question Answering, Natural Language Inference, etc.

You already interact with BERT in several of Google's products. For example, if you ever did a Google search, Google runs your search query through BERT to return the best results to you. This [article](https://blog.google/products/search/search-language-understanding-bert/) runs through several examples of how BERT has made significant improvements on the search results as it can better understand the contextual meaning of words. 

__How exactly does BERT yield better search returns?__

BERT is deeply bidirectional because it utilizes two techniques.

### Masked Language Model (MLM)

Each input replaces 15% of the words. 80% of the replacement is a [MASK] token, 10% is a random token, and 10% will keep the original word. Then the model optimizes the loss function to predict the masked words. 

The BERT loss function takes into consideration only the prediction of the masked values and ignores the prediction of the non-masked words. As a consequence, the model converges slower than directional models, a characteristic which is offset by its increased context awareness.

Another "downside" (feature of the model that you can argue as good or bad) is the mismatch between pre-training and fine-tuning. Since we don't mask tokens during fine-tuning, we add random tokens and original tokens to the masking to obtain vector representations during fine-tuning.

### Next Sentence Prediction (NSP)

Each input is a pair of sentences. The model optimizes to predict if the second sentence in the pair is the subsequent sentence in the original document. 50% of the time, the pair is taken from the inputs, and the other 50% is a randomly generated pair of sentences. 
