---
layout: post
title: Adaboost from scratch
tags: [bert, jekyll, blog, stocks]
image: '/images/posts/pexels-pineapple-supply-co-139259.jpg'
---

thoughts

weak learner and in adaboost called decision stump

decision stump is decision tree with 1 split
so 1 feature and 1 threshold

error is misclassifications over samples

or sum of the weights of the misclassifications

note if error is > 0.5 then we simply flip the decision and the error = 1 - error


Weights

initial weights is 1/N for each sample right so uniform

then weights = (old weight * exponential of (- alpha * y * predicted))/sum of old weights

so misclassified weights have higher weights


Performance

alpha = 0.5 * log((1-error)/error)


Prediction

y = sign of (sum of each prediction * alpha)

so the better our classifier the more impact it has on our final prediction
and the better the classifier the more it points to the negative or positive side



Training

1. Initialize each weight for each sample 1/N
2. Define number of weak learners
3. Loop over
4. Each train classifier find the best threshold and best threshold
5. Calculate error of sum of weights of misclassified
6. Calculate alpha
7. Update weights


```
import numpy as np

class DecisionStump:
  def __init__(self):
    self.polarity = 1 #class should be classified 1 or -1
    self.feature_index = None
    self.split_threshold = None
    self.alpha = None

  def predict(self, X):
    n_samples = X.shape[0]
    X_column = X[:,self.feature_index]

    prediction = np.ones(n_samples) #default to ones
    if self.polarity == 1:
      predictions[X_column < self.split_threshold] = -1
    else:
      predictions[X_column > self.split_threshold] = -1


    return predictions


class AdaBoost:
  def __init__(self, n_clf = 5):
    self.n_clf = n_clf

  def fit(self, X, y):
    n_samples, n_features = X.shape

    #init weights
    w = np.full(n_samples, (1/n_samples)) #sets each value to size n_samples

    self.clfs = []
    for _ in range(self.clf):
      clf = DecisionStump()

      min_error = float('inf')
      eps = 1e-10
      for feature in range(n_features):
        X_column = X[:,feature]
        thresholds = np.unique(X_column)
        for threshold in thresholds:
          p = 1 #polarity
          predictions = np.ones(n_samples)
          predictions[X_column < threshold] = -1

          misclassified = w[y != predictions]
          error = sum(misclassified)
          if error > 0.5:
            error = 1- error
            p = -1

          if error < min_error:
            min_error = error
            clf.polarity = p
            clf.split_threshold = threshold
            clf.feature_index = feature
        clf.alpha = 0.5 * np.log((1-error)/(error+eps))
        predictions = clf.predict(X)
        w *= np.exp(-clf.alpha * y * predictions)
        w /= np.sum(w)
        self.clfs.append(clf)


  def predict(self, X):
    clf_preds = [clf.alpha * clf.predict(X) for clf in self.clfs]
    y_pred = np.sum(clf_preds, axis=0)
    y_pred = np.sign(y_pred)
    return y_pred
